# This is the configuration file for the MAPPO algorithm.

train:
  # number of parallel environments for training data collection
  n_rollout_threads: 32
  # number of steps per environment per training data collection
  episode_length: 100
  # whether to use linear learning rate decay
  use_linear_lr_decay: False
  # whether to consider the case of truncation when an episode is done
  use_proper_time_limits: False
  #by default False, use PopArt to normalize rewards
  use_popart: False
eval:
  # whether to use evaluation
  use_eval: True
  # number of parallel environments for evaluation
  n_eval_rollout_threads: 32
  # number of episodes per evaluation
  eval_episodes: 32
model:
  # network parameters
  #Number of input frames which should be stack together
  stacked_frames: 1
  #Number of layers for actor/critic networks
  layer_N: 1
  #Number of head
  n_head: 4
  # whether to use PMA
  use_pma_block: True
  #Number of seed vector
  n_seed_vector: 1
  # hidden sizes for mlp module in the network
  hidden_size: 32
  # activation function
  use_ReLU: True
  # whether to use feature normalization
  use_feature_normalization: True
  # initialization method for network parameters, choose from orthogonal_, ...
  use_orthogonal: True
  # gain of the output layer of the network.
  gain: 0.01
  # recurrent parameters
  # whether to use rnn policy (data is chunked for training)
  use_recurrent_policy: False
  # number of recurrent layers
  recurrent_N: 1
  # actor learning rate
  lr: 0.0005
  # critic learning rate
  critic_lr: 0.00005
  # eps in Adam
  opti_eps: 0.00001
  # weight_decay in Adam
  weight_decay: 0
  # parameters of diagonal Gaussian distribution
algo:
  # ppo parameters
  # number of epochs for actor Fpdate
  ppo_epoch: 5
  # whether to use clipped value loss
  use_clipped_value_loss: True
  # clip parameter
  clip_param: 0.2
  # number of mini-batches per epoch for update
  num_mini_batch: 1
  # coefficient for entropy term in actor loss
  entropy_coef: 0.01
  # length of data chunk; only useful when use_recurrent_policy is True; episode_length has to be a multiple of data_chunk_length
  data_chunk_length: 10
  # coefficient for value loss
  value_loss_coef: 1
  # whether to clip gradient norm
  use_max_grad_norm: True
  # max gradient norm
  max_grad_norm: 10.0
  # whether to use Generalized Advantage Estimation (GAE)
  use_gae: True
  # discount factor
  gamma: 0.99
  # GAE lambda
  gae_lambda: 0.95
  # whether to use huber loss
  use_huber_loss: True
  # whether to use policy active masks
  use_policy_active_masks: True
  #by default True, whether to mask useless data in value loss.
  use_value_active_masks: False
  # huber delta
  huber_delta: 10.0
  # whether to use ValueNorm
  use_valuenorm: True