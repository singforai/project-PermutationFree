share_observation_space:  [Box(120,), Box(120,), Box(120,), Box(120,), Box(120,)]
observation_space:  [Box(82,), Box(82,), Box(82,), Box(82,), Box(82,)]
action_space:  [Discrete(11), Discrete(11), Discrete(11), Discrete(11), Discrete(11)]
113520
MultiAgentSetTransformer(
  (encoder): Encoder(
    (base): MLPBase(
      (feature_norm): LayerNorm((82,), eps=1e-05, elementwise_affine=True)
      (mlp): MLPLayer(
        (fc1): Sequential(
          (0): Linear(in_features=82, out_features=64, bias=True)
          (1): ReLU()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (fc2): ModuleList(
          (0): Sequential(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): ReLU()
            (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (rnn_layer): RNNLayer(
      (rnn): GRU(64, 64)
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (SAB_block1): Sequential(
      (0): SetAttentionBlock(
        (mab): MultiheadAttentionBlock(
          (multihead): MultiheadAttention(
            (project_queries): Linear(in_features=64, out_features=64, bias=True)
            (project_keys): Linear(in_features=64, out_features=64, bias=True)
            (project_values): Linear(in_features=64, out_features=64, bias=True)
            (concatenation): Linear(in_features=64, out_features=64, bias=True)
            (attention): Attention(
              (softmax): Softmax(dim=2)
            )
          )
          (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (layer_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (rff): RFF(
            (layers): Sequential(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): ReLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): ReLU()
            )
          )
        )
      )
    )
    (SAB_block2): Sequential(
      (0): SetAttentionBlock(
        (mab): MultiheadAttentionBlock(
          (multihead): MultiheadAttention(
            (project_queries): Linear(in_features=64, out_features=64, bias=True)
            (project_keys): Linear(in_features=64, out_features=64, bias=True)
            (project_values): Linear(in_features=64, out_features=64, bias=True)
            (concatenation): Linear(in_features=64, out_features=64, bias=True)
            (attention): Attention(
              (softmax): Softmax(dim=2)
            )
          )
          (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (layer_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (rff): RFF(
            (layers): Sequential(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): ReLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): ReLU()
            )
          )
        )
      )
    )
    (act_layer): ACTLayer(
      (action_out): Categorical(
        (linear): Linear(in_features=64, out_features=11, bias=True)
      )
    )
  )
  (decoder): Decoder(
    (PMA_block): Sequential(
      (0): PoolingMultiheadAttention(
        (mab): MultiheadAttentionBlock(
          (multihead): MultiheadAttention(
            (project_queries): Linear(in_features=64, out_features=64, bias=True)
            (project_keys): Linear(in_features=64, out_features=64, bias=True)
            (project_values): Linear(in_features=64, out_features=64, bias=True)
            (concatenation): Linear(in_features=64, out_features=64, bias=True)
            (attention): Attention(
              (softmax): Softmax(dim=2)
            )
          )
          (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (layer_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (rff): RFF(
            (layers): Sequential(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): ReLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): ReLU()
            )
          )
        )
      )
    )
    (v_net): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
      (2): Linear(in_features=32, out_features=1, bias=True)
    )
  )
)
<bound method Module.parameters of MultiAgentSetTransformer(
  (encoder): Encoder(
    (base): MLPBase(
      (feature_norm): LayerNorm((82,), eps=1e-05, elementwise_affine=True)
      (mlp): MLPLayer(
        (fc1): Sequential(
          (0): Linear(in_features=82, out_features=64, bias=True)
          (1): ReLU()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (fc2): ModuleList(
          (0): Sequential(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): ReLU()
            (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (rnn_layer): RNNLayer(
      (rnn): GRU(64, 64)
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (SAB_block1): Sequential(
      (0): SetAttentionBlock(
        (mab): MultiheadAttentionBlock(
          (multihead): MultiheadAttention(
            (project_queries): Linear(in_features=64, out_features=64, bias=True)
            (project_keys): Linear(in_features=64, out_features=64, bias=True)
            (project_values): Linear(in_features=64, out_features=64, bias=True)
            (concatenation): Linear(in_features=64, out_features=64, bias=True)
            (attention): Attention(
              (softmax): Softmax(dim=2)
            )
          )
          (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (layer_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (rff): RFF(
            (layers): Sequential(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): ReLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): ReLU()
            )
          )
        )
      )
    )
    (SAB_block2): Sequential(
      (0): SetAttentionBlock(
        (mab): MultiheadAttentionBlock(
          (multihead): MultiheadAttention(
            (project_queries): Linear(in_features=64, out_features=64, bias=True)
            (project_keys): Linear(in_features=64, out_features=64, bias=True)
            (project_values): Linear(in_features=64, out_features=64, bias=True)
            (concatenation): Linear(in_features=64, out_features=64, bias=True)
            (attention): Attention(
              (softmax): Softmax(dim=2)
            )
          )
          (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (layer_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (rff): RFF(
            (layers): Sequential(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): ReLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): ReLU()
            )
          )
        )
      )
    )
    (act_layer): ACTLayer(
      (action_out): Categorical(
        (linear): Linear(in_features=64, out_features=11, bias=True)
      )
    )
  )
  (decoder): Decoder(
    (PMA_block): Sequential(
      (0): PoolingMultiheadAttention(
        (mab): MultiheadAttentionBlock(
          (multihead): MultiheadAttention(
            (project_queries): Linear(in_features=64, out_features=64, bias=True)
            (project_keys): Linear(in_features=64, out_features=64, bias=True)
            (project_values): Linear(in_features=64, out_features=64, bias=True)
            (concatenation): Linear(in_features=64, out_features=64, bias=True)
            (attention): Attention(
              (softmax): Softmax(dim=2)
            )
          )
          (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (layer_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (rff): RFF(
            (layers): Sequential(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): ReLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): ReLU()
            )
          )
        )
      )
    )
    (v_net): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
      (2): Linear(in_features=32, out_features=1, bias=True)
    )
  )
)>
